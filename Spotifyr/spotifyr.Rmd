---
title: "Spotify Genre Classifier"
author: "Andrew Lee"
output: html_document
---

<br />
<br />
<br />

Spotify have gained popularity for their music streaming service since they were launched in 2008. They stream assorted genre of songs, such as Hip-hop, Dance, Latin, etc. In this project, I built a model to classify songs from different genres. Songs were selected based on being on the top billboard charts in the last 11 years, compiling a list of 3544 songs in the five genres; 950 country, 868 dance, 751 latin, 883 R&B/HipHop, and 92 rock songs. I chose to use year end charts to avoid having the same song in the dataset multiple times. The data was gathered using a Python script, with packages beautifulsoup and pandas, to read the billboard.com year-end chart pages and store the title and artist information. The script then used the title and artist to obtain the URI for Spotify. Once the script finished running, the data was output to a CSV file to be read by R. Once in R, the spotifyr package was used to collect the features about each song. The features collected are:

<br />
<br />

* **key**: _int_ The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.
* **time_signature**: _int_	An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).
* **acousticness**: _float_ A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
* **danceability**: _float_	Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. 
* **energy**: _float_	Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
* **instrumentalness**: _float_	Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. 
* **liveness**: _float_	Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. 
* **loudness**: _float_	The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.
* **speechiness**: _float_ Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.
* **valence**: _float_ A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
* **tempo**: _float_ The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

<br />

Neural networks are models used for both classification and prediction. They mimic the way that human brain learns from experience. The main strength of neural networks is their high predictive performance. Their structure captures very complex relationships between predictors and response. 

```{r}
library(dplyr)
library(caTools)
library(neuralnet)

songs = read.csv("all_song_data.csv")
```

# Data Preprocessing

<br />

```{r}
# retain only the necessary variables

rnames = paste(songs$Title, songs$Artist, sep = " by ")
rownames(songs) = make.names(rnames, unique = TRUE)

songs = songs[,c(2,6:16)]
```

Neural networks perform best when the predictors and response variables are on a scale of [0, 1]. For this reason, all variables should be scaled before fitting the model.

```{r}
summary(songs)
```

Thankfully, in this project, most of the variables that we are going to use are bounded from 0 to 1. However, as we can see above, key, loudness, tempo, and time_signature are not bounded to the range. For those numerical predictors, we can normalize the measurements by subtracting minimum and dividing by the range. For example, the minimum of tempo is 48.72 and the maximum is 212.06. So, calculate normalized tempo as $\frac {tempo - 48.72} {212.06}$. We can apply this to tempo, loudness, and time_signature.

```{r}
songs$tempo = (songs$tempo-min(songs$tempo))/(max(songs$tempo)-min(songs$tempo))
songs$loudness = (songs$loudness-min(songs$loudness))/(max(songs$loudness)-min(songs$loudness))
songs$time_signature = (songs$time_signature-min(songs$time_signature))/(max(songs$time_signature)-min(songs$time_signature))
```

Although it is incoded as numerical for now, key is categorical variables. Since they are ordinal in nature, a choice of fractions in [0, 1] should reflect their percieved ordering.

```{r}
unique(sort(songs$key))
1/(length(unique(songs$key))-1)
```

```{r}
songs$key = ifelse(songs$key == 0, 0,
                   ifelse(songs$key == 1, 0.09090909,
                          ifelse(songs$key == 2, 0.1818182,
                                 ifelse(songs$key == 3, 0.2727273,
                                        ifelse(songs$key == 4, 0.3636364,
                                               ifelse(songs$key == 5, 0.4545455,
                                                      ifelse(songs$key == 6, 0.5454545,
                                                             ifelse(songs$key == 7, 0.6363636,
                                                                    ifelse(songs$key == 8, 0.7272727,
                                                                           ifelse(songs$key == 9, 0.8181818,
                                                                                  ifelse(songs$key == 10, 0.9090909, 1)))))))))))
```

For the response, Genre, since it has 5 different categories and nominal, we should encode it into 5 vectors: where, for each genre, 1 represents it belongs to the genre and 0 represents it does not.

```{r}
songs = mutate(songs, country = ifelse(songs$Genre == "country", 1, 0))
songs = mutate(songs, dance = ifelse(songs$Genre == "dance", 1, 0))
songs = mutate(songs, latin = ifelse(songs$Genre == "latin", 1, 0))
songs = mutate(songs, rnb.hiphop = ifelse(songs$Genre == "r&b/hiphop", 1, 0))
songs = mutate(songs, rock = ifelse(songs$Genre == "rock", 1, 0))

```

```{r}
summary(songs)
```

# Fitting the model with neuralnet

```{r}
# Spliting Dataset
set.seed(1123) 
split = sample.split(songs$Genre, SplitRatio = 0.75) 

songs.nnet = songs[,-1]
songs.train = subset(songs.nnet, split == TRUE) 
songs.test = subset(songs.nnet, split == FALSE) 
```

```{r}
# Set up formula
n = names(songs.train)
f = as.formula(paste("country + dance + latin + rnb.hiphop + rock ~", paste(n[!n %in% c("country","dance","latin","rnb.hiphop","rock")], collapse = " + ")))
f
```

### Required User Input

<br />

* **Number of Hidden Layers:** The most popular choice for the number of hidden layer is one. A single hidden layer is usually sufficient to capture even very complex relationships between predictors. Model with more than one hidden layer is called deep learning.
* **Size of Hidden Layers:** The Number of nodes in the hidden layer. It determines the level of complexity of the relationship between the predictors that the network captures. The trade-off is between under- and overfitting. Using too few nodes might not be sufficient to capture complex relationships. On the other hand, too many nodes might lead to overfitting. A rule of thumb is to start with the nodes of the number of predictors and gradually decrease or increase a bit while checking for overfitting.
* **Link function:** Logistic/sigmoidal function is one of the most popular one in neural networks. Its practical value arises from the fact that it has squashing effect on very small or very large valeus bue is almost linear in the range where the value of the function is between 0.1 and 0.9.
    
<br />

```{r}
nn = neuralnet(f, 
               data = songs.train, 
               threshold = 0.2,
               hidden = 11,
               act.fct = "logistic", 
               linear.output = FALSE, 
               lifesign = "none")
```

### Let’s have a look at the accuracy on the training set:

```{r}
# Compute predictions
pr.nn = compute(nn, songs.train[,1:11])

# Extract results
pr.nn_ = pr.nn$net.result

# Accuracy (training set)
original_values = max.col(songs.train[,12:16])
pr.nn_2 = max.col(pr.nn_)
accuracy = mean(pr.nn_2 == original_values)

accuracy
```

With the training set, the model classifies about 75% of the songs correctly. To see the accuracy on test set, let's perform cross validation!

# Cross Validation
```{r}
set.seed(1123)

# 5 fold cross validation
k = 5
# Results from cv
outs = NULL
# Train test split proportions
p = 0.75

# Crossvalidate, go!
for(i in 1:k)
{
    split = sample.split(songs$Genre, SplitRatio = p) 
    train_cv = subset(songs.nnet, split == TRUE)
    test_cv = subset(songs.nnet, split == FALSE)
    nn_cv = neuralnet(f, 
                      data = train_cv, 
                      threshold = 0.2,
                      hidden = 11,
                      act.fct = "logistic", 
                      linear.output = FALSE, 
                      lifesign = "none")

    # Compute predictions
    pr.nn = compute(nn_cv, test_cv[,1:11])
    # Extract results
    pr.nn_ = pr.nn$net.result
    # Accuracy (test set)
    original_values = max.col(test_cv[,12:16])
    pr.nn_2 = max.col(pr.nn_)
    outs[i] = mean(pr.nn_2 == original_values)
}

mean(outs)
```

On average, the model shows 70% of accuracy from the test data.

Project files:
<https://github.com/aleedk1123/spotifyr>